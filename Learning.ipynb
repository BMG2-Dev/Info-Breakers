{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Team Information\n"
      ],
      "metadata": {
        "id": "B2voleaIBFll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wildfire Dataset Analysis\n",
        "### Class: 4330 | Team: Infobreakers | Braeden Gutierrez, Antonio Rodriguez , Josue Lozano\n",
        "- **Contact Information**: arodriguez265@angelo.edu, jlozano18@angelo.edu, bgutierrez14@angelo.edu\n",
        "- **Dataset**: This dataset contains wildfire occurrence information, including the total acres burned, year of occurrence, and fire size classification."
        "- **GitHub**: https://github.com/BMG2-Dev/Info-Breakers/blob/main/Learning.ipynb"
      ],
      "metadata": {
        "id": "xhm-vv6CBJAM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3DOKwYsIiVK"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XuSofqOSHbQx"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import geopandas as gp  # for spatial data handling\n",
        "import pandas as pd  # for data handling\n",
        "import numpy as np  # for numerical operations\n",
        "import seaborn as sns  # for visualization\n",
        "import matplotlib.pyplot as plt  # for plotting\n",
        "import re\n",
        "\n",
        "from shapely.geometry import Point  # for creating geographic points\n",
        "from google.colab import drive  # for mounting Google Drive in Colab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvbJYOYpIr64"
      },
      "source": [
        "#Data Use Location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dhHfOSI_TXr",
        "outputId": "68a518f5-b333-45e0-ca44-7831f4d99953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Specifies that we can use data in our Google Drive under '/content/drive'.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzRPHkD4I9xK"
      },
      "source": [
        "#National Fire Datasets Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kQj0cCYTbCll"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import geopandas as gp\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Suppress standard output temporarily\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "# Your code within the suppress context\n",
        "with SuppressOutput():\n",
        "    # Load the National fires occurrences dataset\n",
        "    df = pd.read_csv(\n",
        "        '/content/drive/MyDrive/CS4330/National_USFS_Fire_Occurrence_Point_(Feature_Layer).csv',\n",
        "        low_memory=False\n",
        "    )\n",
        "\n",
        "    # Create geometric points for fire locations\n",
        "    fires = [Point(long_lad) for long_lad in zip(df['X'], df['Y'])]\n",
        "\n",
        "    # Create a spatial-enabled GeoDataFrame using the dataset\n",
        "    loc_df = gp.GeoDataFrame(df, geometry=fires, crs=\"EPSG:4326\")\n",
        "\n",
        "# The `loc_df` GeoDataFrame is now processed and ready for further use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzb7S7w1KL8x"
      },
      "source": [
        "#US Boundry Datasets Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UroR3sArCRyK"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import geopandas as gp\n",
        "\n",
        "# Suppress standard output temporarily\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "# Your code within the suppress context\n",
        "with SuppressOutput():\n",
        "    # Load US census data for county and state geometric boundaries\n",
        "    county_shapes = gp.read_file('/content/drive/MyDrive/CS4330/tl_2024_us_county/tl_2024_us_county.shp')\n",
        "    state_shapes = gp.read_file('/content/drive/MyDrive/CS4330/tl_2024_us_state/tl_2024_us_state.shp')\n",
        "\n",
        "    # Convert the coordinate system to EPSG:4326\n",
        "    county_shapes = county_shapes.to_crs(\"EPSG:4326\")\n",
        "    state_shapes = state_shapes.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# The data is now loaded and transformed; you can inspect the variables `county_shapes` and `state_shapes` outside the block if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfwTAsO_LGjd"
      },
      "source": [
        "#Creation of Counties Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ca_lXa60CbMW"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import geopandas as gp\n",
        "\n",
        "# Suppress standard output temporarily\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "# Your code within the suppress context\n",
        "with SuppressOutput():\n",
        "    # Spatial join of county boundary data and fire locations\n",
        "    counties = gp.sjoin(loc_df, county_shapes, how=\"left\", predicate=\"intersects\")\n",
        "\n",
        "    # Rename column 'NAME' to 'COUNTY'\n",
        "    counties.rename(columns={'NAME': 'COUNTY'}, inplace=True)\n",
        "\n",
        "    # Select relevant columns for the counties DataFrame\n",
        "    counties = counties[['Y', 'X', 'COUNTY', 'FIREYEAR', 'SIZECLASS', 'TOTALACRES', 'STATCAUSE', 'FIRETYPECATEGORY']]\n",
        "\n",
        "# The `counties` DataFrame is now ready for further use without text output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2kb1rBIRL5P"
      },
      "source": [
        "#Creation of States Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "B6Y6GOwRRLTU"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import geopandas as gp\n",
        "\n",
        "# Suppress standard output temporarily\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "# Your code within the suppress context\n",
        "with SuppressOutput():\n",
        "    # Spatial join of state boundary data and fire locations\n",
        "    states = gp.sjoin(loc_df, state_shapes, how=\"left\", predicate=\"intersects\")\n",
        "\n",
        "    # Rename columns and select relevant fields\n",
        "    states.rename(columns={'NAME': 'STATE'}, inplace=True)\n",
        "    states = states[['Y', 'X', 'STATE', 'FIREYEAR', 'SIZECLASS', 'TOTALACRES', 'STATCAUSE', 'FIRETYPECATEGORY']]\n",
        "\n",
        "# The `states` DataFrame is now processed and ready for further use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uAqIwK_LdR3"
      },
      "source": [
        "#Merging of Counties and States Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WNwEo5IYCzg8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Suppress standard output temporarily\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "# Your code within the suppress context\n",
        "with SuppressOutput():\n",
        "    # Merge the counties and states datasets\n",
        "    us_fires = pd.merge(\n",
        "        counties,\n",
        "        states,\n",
        "        on=['Y', 'X', 'FIREYEAR', 'SIZECLASS', 'TOTALACRES', 'STATCAUSE', 'FIRETYPECATEGORY'],\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # Rename columns\n",
        "    us_fires = us_fires.rename(columns={'X': 'LONGITUDE', 'Y': 'LATITUDE'})\n",
        "\n",
        "# At this point, warnings will still show up, but no text output is displayed\n",
        "# You can now use `us_fires` as needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1uLJJby9sgM"
      },
      "source": [
        "#Dataset Error and Formating Changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pIANrrXXL9UO"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Suppress standard output temporarily\n",
        "class SuppressOutput:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "# Your full code within the suppress context\n",
        "with SuppressOutput():\n",
        "    # Fill missing data and specify column type\n",
        "    us_fires['FIREYEAR'] = us_fires['FIREYEAR'].fillna(-1).astype(int)\n",
        "\n",
        "    # Replace string abbreviations and fix typos\n",
        "    us_fires.loc[:, 'FIRETYPECATEGORY'] = us_fires['FIRETYPECATEGORY'].str.replace('WF', 'WILDFIRE', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('Undertermined', 'Undetermined', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace(' Undetermined', 'Undetermined', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('Firearms/Weapons ', 'Firearms/Weapons', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('9 -  Miscellaneous', 'Miscellaneous', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('9 - Miscellaneous', 'Miscellaneous', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('5 -  Debris Burning', 'Debris Burning', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('Camping ', 'Camping', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('1 - Lightning', 'Lightning', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('4 - Campfire', 'Campfire', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('5 - Debris burning', 'Debris burning', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('7-Arson', 'Arson', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('5-Debris burning', 'Debris burning', case=False)\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].str.replace('[0-9]', 'Unknown', regex=True)\n",
        "\n",
        "    # Fill missing data\n",
        "    us_fires.loc[:, 'COUNTY'] = us_fires['COUNTY'].fillna('Unknown')\n",
        "    us_fires.loc[:, 'STATE'] = us_fires['STATE'].fillna('Unknown')\n",
        "    us_fires.loc[:, 'STATCAUSE'] = us_fires['STATCAUSE'].fillna('Unknown')\n",
        "    us_fires.loc[:, 'FIRETYPECATEGORY'] = us_fires['FIRETYPECATEGORY'].fillna('Unknown')\n",
        "    us_fires.loc[:, 'TOTALACRES'] = us_fires['TOTALACRES'].fillna(-1)\n",
        "\n",
        "    # Define fire size categories and assign size classes\n",
        "    fire_sizes = [\n",
        "        (us_fires['TOTALACRES'] >= 0.00) & (us_fires['TOTALACRES'] <= 0.25),\n",
        "        (us_fires['TOTALACRES'] >= 0.26) & (us_fires['TOTALACRES'] <= 9.99),\n",
        "        (us_fires['TOTALACRES'] >= 10.00) & (us_fires['TOTALACRES'] <= 99.99),\n",
        "        (us_fires['TOTALACRES'] >= 100.00) & (us_fires['TOTALACRES'] <= 299.99),\n",
        "        (us_fires['TOTALACRES'] >= 300.00) & (us_fires['TOTALACRES'] <= 999.99),\n",
        "        (us_fires['TOTALACRES'] >= 1000.00) & (us_fires['TOTALACRES'] <= 4999.99),\n",
        "        (us_fires['TOTALACRES'] >= 5000.00) & (us_fires['TOTALACRES'] <= 9999.99),\n",
        "        (us_fires['TOTALACRES'] >= 10000.00) & (us_fires['TOTALACRES'] <= 49999.99),\n",
        "        (us_fires['TOTALACRES'] >= 50000.00) & (us_fires['TOTALACRES'] <= 99999.99),\n",
        "        (us_fires['TOTALACRES'] >= 100000.00) & (us_fires['TOTALACRES'] <= 499999.99),\n",
        "        (us_fires['TOTALACRES'] >= 500000.00) & (us_fires['TOTALACRES'] <= 999999.99),\n",
        "        (us_fires['TOTALACRES'] >= 1000000.00)\n",
        "    ]\n",
        "\n",
        "    fire_size_class = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
        "    us_fires['SIZECLASS'] = np.select(fire_sizes, fire_size_class, default='Unknown')\n",
        "\n",
        "    # Example filtering operation (adjusted)\n",
        "    filtered_data = us_fires[(us_fires['FIRETYPECATEGORY'] != 'WILDFIRE') & (us_fires['FIRETYPECATEGORY'] != ' ')]\n",
        "\n",
        "# Outside the suppress block, warnings are still active\n",
        "# Use filtered_data or inspect any variable as needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MsBCFg1zae9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised"
      ],
      "metadata": {
        "id": "eJ0WmZynouNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning: MiniBatchKMeans Clustering\n",
        "### Goal:\n",
        "To group wildfires based on `TOTALACRES` and `FIREYEAR` into meaningful clusters and evaluate the clustering performance using the Silhouette Score.\n"
      ],
      "metadata": {
        "id": "-JtLwmWRBcoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Sample a subset of the dataset\n",
        "# Changed 'data' to 'us_fires' assuming this is the DataFrame containing fire data.\n",
        "sampled_data = us_fires.sample(n=10000, random_state=42)  # Reduce dataset size\n",
        "features = sampled_data[['TOTALACRES', 'FIREYEAR']]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Apply MiniBatchKMeans with optimized parameters\n",
        "mbkmeans = MiniBatchKMeans(n_clusters=4, random_state=42, batch_size=1000, max_iter=100)\n",
        "clusters = mbkmeans.fit_predict(features_scaled)\n",
        "\n",
        "# Evaluate clustering\n",
        "sil_score = silhouette_score(features_scaled, clusters)\n",
        "print(f\"Silhouette Score: {sil_score}\")\n",
        "\n",
        "# Add cluster labels to the dataset\n",
        "sampled_data['Cluster'] = clusters\n",
        "print(sampled_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI-wLDM2pAWI",
        "outputId": "cc9f7322-040c-4fa6-a755-862cd4101b1b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.44421127982321307\n",
            "         LATITUDE   LONGITUDE    COUNTY  FIREYEAR SIZECLASS  TOTALACRES  \\\n",
            "194667  35.094787 -112.184944  Coconino      1970         A         0.1   \n",
            "182820  34.799430 -111.437838  Coconino      1971         A         0.1   \n",
            "77389   46.830000 -113.925000  Missoula      2011         A         0.1   \n",
            "268805  34.776467  -83.722717     White      2021         B         2.0   \n",
            "52630   47.758533 -113.818000      Lake      2023         A         0.2   \n",
            "\n",
            "                  STATCAUSE FIRETYPECATEGORY    STATE  Cluster  \n",
            "194667            Lightning         WILDFIRE  Arizona        2  \n",
            "182820            Lightning         WILDFIRE  Arizona        2  \n",
            "77389               Camping         WILDFIRE  Montana        1  \n",
            "268805              Unknown         WILDFIRE  Georgia        1  \n",
            "52630   Other Natural Cause         WILDFIRE  Montana        1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "- **Silhouette Score**: `0.444`\n",
        "  - Indicates moderate clustering performance. Higher values (closer to `1`) would represent better-defined clusters.\n",
        "- The dataset now includes a `Cluster` column, assigning each wildfire to one of the four clusters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BZt-gAgnBfIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised"
      ],
      "metadata": {
        "id": "_h1yiJ2bpDAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Learning: Support Vector Machine (SVM)\n",
        "### Goal:\n",
        "To classify wildfire size (`SIZECLASS`) based on `TOTALACRES` and `FIREYEAR` using a Support Vector Machine (SVM) and evaluate its performance.\n"
      ],
      "metadata": {
        "id": "xVIcm9i1BlDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "!pip install -q imbalanced-learn\n",
        "\n",
        "# Assume 'us_fires' is the DataFrame you loaded and preprocessed\n",
        "data = us_fires\n",
        "\n",
        "# Use features and SIZECLASS as input and output\n",
        "features = data[['TOTALACRES', 'FIREYEAR']]\n",
        "labels = data['SIZECLASS']\n",
        "\n",
        "# Sample a larger subset of the dataset\n",
        "sample_size = 10000\n",
        "sampled_data = data.sample(n=sample_size, random_state=42)\n",
        "features_sampled = sampled_data[['TOTALACRES', 'FIREYEAR']]\n",
        "labels_sampled = sampled_data['SIZECLASS']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features_sampled)\n",
        "\n",
        "# Balance the dataset using SMOTE with k_neighbors adjusted\n",
        "# The k_neighbors parameter is set to the minimum between 5 and the\n",
        "# number of samples in the smallest class, preventing the ValueError.\n",
        "smote = SMOTE(random_state=42, k_neighbors=min(5, labels_sampled.value_counts().min() - 1))\n",
        "features_balanced, labels_balanced = smote.fit_resample(features_scaled, labels_sampled)\n",
        "\n",
        "# Split the balanced data into training and testing sets\n",
        "# Added the train_test_split line with a test size of 0.2 (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_balanced, labels_balanced, test_size=0.2, random_state=42)\n",
        "# Train and evaluate the SVM model\n",
        "svm_model = SVC(kernel='linear', random_state=42, class_weight='balanced')\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_qlIEHrpGTn",
        "outputId": "c9783aa3-bc46-4e83-daaf-a91dd482dd06"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[  10 1248    0    0    0    0    0    0    0    0    2]\n",
            " [   0 1271    0    0    0    0    0    0    0    0    0]\n",
            " [  67  881  299   51    0    0    0    0    0    0    0]\n",
            " [   0    0    0 1247    4    0    0    0    0    0    0]\n",
            " [   0    0    0    0 1314    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0 1303    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0 1268    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0 1255    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0 1266    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0 1299    0]\n",
            " [ 211  973    0    0    0    0    0    0    0    0   67]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.03      0.01      0.01      1260\n",
            "           B       0.29      1.00      0.45      1271\n",
            "           C       1.00      0.23      0.37      1298\n",
            "           D       0.96      1.00      0.98      1251\n",
            "           E       1.00      1.00      1.00      1314\n",
            "           F       1.00      1.00      1.00      1303\n",
            "           G       1.00      1.00      1.00      1268\n",
            "           H       1.00      1.00      1.00      1255\n",
            "           I       1.00      1.00      1.00      1266\n",
            "           J       1.00      1.00      1.00      1299\n",
            "     Unknown       0.97      0.05      0.10      1251\n",
            "\n",
            "    accuracy                           0.76     14036\n",
            "   macro avg       0.84      0.75      0.72     14036\n",
            "weighted avg       0.84      0.76      0.72     14036\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "- **Confusion Matrix**: Shows the distribution of predictions versus actual labels for each size class.\n",
        "- **Classification Report**:\n",
        "  - **Accuracy**: `0.76` indicates that 76% of predictions matched the true labels.\n",
        "  - **Challenges**: Certain size classes (e.g., `A`) have lower precision and recall, likely due to imbalanced representation.\n"
      ],
      "metadata": {
        "id": "KI2uBXVtBnRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "O4qPle9RBr-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "- **Unsupervised Learning**:\n",
        "  - MiniBatchKMeans effectively grouped wildfires into clusters with a moderate Silhouette Score of `0.444`.\n",
        "  - Clusters are based on total acres burned and the year of occurrence.\n",
        "- **Supervised Learning**:\n",
        "  - SVM achieved an accuracy of `0.76` for classifying wildfire sizes.\n",
        "  - Class imbalance posed challenges, addressed using SMOTE for balancing.\n",
        "- **Key Takeaways**:\n",
        "  - Feature scaling and data balancing are critical for effective machine learning.\n",
        "  - Both supervised and unsupervised learning provided insights into wildfire patterns.\n"
      ],
      "metadata": {
        "id": "Pe32tQO4BukF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VvbJYOYpIr64",
        "QzRPHkD4I9xK",
        "Xzb7S7w1KL8x",
        "BfwTAsO_LGjd",
        "L2kb1rBIRL5P",
        "1uAqIwK_LdR3",
        "X1uLJJby9sgM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
